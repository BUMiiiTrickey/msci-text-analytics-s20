<img width="993" alt="Screen Shot 2020-07-03 at 11 51 33 AM" src="https://user-images.githubusercontent.com/65793583/86484173-9fc1ba80-bd23-11ea-90a3-27fac05ada12.png">

According to the accuracy score we got above, we can find out that the best accuracy score obtained when we use ReLu as activation with droupout layer, and the lowest accuracy score occured when we use signmoid as activation with L2 regularization and dropout layer. Obvisouly, model with ReLu activation function got better accuracy score than other model did since ReLu involve expensive operations compare to sigmoid and tanh activation, also it can be implemented by simply thresholding a matrix of activations at zero.With applying the L2-norm regularization, it allows some training sample to be misclassified and reduces the overfitting therefore it slightly improve the accuracy score of model with sigmoid activation.With applying dropout layer,it also helps us to prevent model from overfitting, so the score of model with dropout layer is the highest in each model.
